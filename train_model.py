"""
    Name:           inception_train.py
    Created:        2/4/2017
    Description:    Fine-tune inception v3 on specific data.
"""
#==============================================
#                   Modules
#==============================================
import sys
import os
import numpy as np
import pandas as pd
import time
import gzip
import pickle
from collections import Counter
from keras.applications.inception_v3 import InceptionV3
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from keras.models import Model, model_from_json
from keras.layers import Dense, GlobalAveragePooling2D
from keras import backend as K
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.applications.imagenet_utils import decode_predictions
from keras.optimizers import SGD
from sklearn.preprocessing import LabelEncoder
import multiprocessing as mp
#==============================================
#                   Files
#==============================================


#==============================================
#                   Functions
#==============================================
def instantiate(n_classes, n_dense=2048, inception_json="inceptionv3_mod.json", verbose=1):
    """
    Instantiate the inception v3.
    """

    # create the base pre-trained model
    base_model = InceptionV3(weights='imagenet', include_top=False)

    # add a global spatial average pooling layer
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    # let's add a fully-connected layer
    x = Dense(n_dense, activation='relu')(x)
    # and a final logistic layer
    predictions = Dense(n_classes, activation='softmax')(x)

    # this is the model we will train
    model = Model(inputs=base_model.input, outputs=predictions)

    # first: train only the top layers (which were randomly initialized)
    # i.e. freeze all convolutional InceptionV3 layers
    for layer in base_model.layers:
        layer.trainable = False

    # compile the model (should be done *after* setting layers to non-trainable)
    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')

    # serialize model to json
    model_json = model.to_json()
    with open(inception_json, "w") as iOF:
        iOF.write(model_json)

    return base_model, model




def finetune(base_model, model, train_dir, val_dir,
             epochs_1=15, epochs_2=30, patience_1=1, patience_2=1, batch_size=32,
             nb_train_samples=85639, nb_validation_samples=10694,
             img_width=299, img_height=299, class_imbalance=True,
             inception_h5_1="inceptionv3_fine_tuned_1.h5", inception_h5_2="inceptionv3_fine_tuned_2.h5",
             inception_h5_check_point_1="inceptionv3_fine_tuned_check_point_1.h5", inception_h5_check_point_2="inceptionv3_fine_tuned_check_point_2.h5",
             layer_names_file="inceptionv3_mod_layer_names.txt", verbose=1):
    """
    Finetune the inception v3.
    """

    # this is the augmentation configuration we will use for training
    train_datagen = ImageDataGenerator(
        preprocessing_function=preprocess_input,
        horizontal_flip=True,
        zoom_range=0.2,
        width_shift_range=0.2,
        height_shift_range=0.2,
        rotation_range=20)

    # this is the augmentation configuration we will use for testing:
    # only rescaling
    test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)

    # define train & val data generators
    train_generator = train_datagen.flow_from_directory(
        train_dir,
        target_size=(img_width, img_height),
        batch_size=batch_size,
        class_mode='categorical')

    validation_generator = test_datagen.flow_from_directory(
        val_dir,
        target_size=(img_width, img_height),
        batch_size=batch_size,
        class_mode='categorical')

    # get class weights
    if class_imbalance:
        class_weight = get_class_weights(train_generator.classes, smooth_factor=0.1)
    else:
        class_weight = None

    if verbose >= 2:
        class_name_dict = {val:key for key,val in train_generator.class_indices.items()}
        print {class_name_dict[key]:val for key,val in class_weight.items()}

    # train the model on the new data for a few epochs on the batches generated by datagen.flow().
    model.fit_generator(
        train_generator,
        steps_per_epoch=nb_train_samples // batch_size,
        epochs=epochs_1,
        validation_data=validation_generator,
        validation_steps=nb_validation_samples // batch_size,
        callbacks=[EarlyStopping(monitor='val_loss', patience=patience_1),
                   ModelCheckpoint(filepath=inception_h5_check_point_1, verbose=1, save_best_only=True)],
        class_weight=class_weight)

    # save weights just in case
    model.save_weights(inception_h5_1)

    # at this point, the top layers are well trained and we can start fine-tuning
    # convolutional layers from inception V3. We will freeze the bottom N layers
    # and train the remaining top layers.

    # let's visualize layer names and layer indices to see how many layers
    # we should freeze:
    with open(layer_names_file, "w") as iOF:
        for ix, layer in enumerate(base_model.layers):
            iOF.write("%d, %s\n"%(ix, layer.name))
            print ix, layer.name

    # we chose to train the top 2 inception blocks, i.e. we will freeze
    # the first 172 layers and unfreeze the rest:
    for layer in model.layers[:172]:
        layer.trainable = False
    for layer in model.layers[172:]:
        layer.trainable = True

    # we need to recompile the model for these modifications to take effect
    # we use SGD with a low learning rate
    model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')

    # we train our model again (this time fine-tuning the top 2 inception blocks
    # alongside the top Dense layers
    model.fit_generator(
        train_generator,
        steps_per_epoch=nb_train_samples // batch_size,
        epochs=epochs_2,
        validation_data=validation_generator,
        validation_steps=nb_validation_samples // batch_size,
        callbacks=[EarlyStopping(monitor='val_loss', patience=patience_2),
                   ModelCheckpoint(filepath=inception_h5_check_point_2, verbose=1, save_best_only=True)],
        class_weight=class_weight)

    # save final weights
    model.save_weights(inception_h5_2)




def finetune_from_saved(inception_h5_load_from, inception_h5_save_to,
             inception_json, train_dir, val_dir, nb_freeze=0,
             epochs=50, patience=2, batch_size=32,
             nb_train_samples=85639, nb_validation_samples=10694,
             img_width=299, img_height=299, class_imbalance=True,
             inception_h5_check_point="inceptionv3_fine_tuned_check_point_3.h5", verbose=1):
    """
    Finetune the inception v3 from already fine-tuned one.
    """

    # load json and create model
    with open(inception_json, 'r') as iOF:
        loaded_model_json = iOF.read()
    loaded_model = model_from_json(loaded_model_json)
    # load weights into new model
    loaded_model.load_weights(inception_h5_load_from)
    if verbose >= 1: print "Loaded model from disk"

    # we freeze the first nb_freeze layers and unfreeze the rest:
    for layer in loaded_model.layers[:nb_freeze]:
        layer.trainable = False
    for layer in loaded_model.layers[nb_freeze:]:
        layer.trainable = True

    # we need to recompile the model for these modifications to take effect
    # we use SGD with a low learning rate
    loaded_model.compile(optimizer=SGD(lr=0.00001, momentum=0.9), loss='categorical_crossentropy')

    # this is the augmentation configuration we will use for training
    train_datagen = ImageDataGenerator(
        preprocessing_function=preprocess_input,
        horizontal_flip=True,
        zoom_range=0.2,
        width_shift_range=0.2,
        height_shift_range=0.2,
        rotation_range=20)

    # this is the augmentation configuration we will use for testing:
    # only rescaling
    test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)

    # define train & val data generators
    train_generator = train_datagen.flow_from_directory(
        train_dir,
        target_size=(img_width, img_height),
        batch_size=batch_size,
        class_mode='categorical')

    validation_generator = test_datagen.flow_from_directory(
        val_dir,
        target_size=(img_width, img_height),
        batch_size=batch_size,
        class_mode='categorical')

    # get class weights
    if class_imbalance:
        class_weight = get_class_weights(train_generator.classes, smooth_factor=0.1)
    else:
        class_weight = None

    # train the model on the new data for a few epochs on the batches generated by datagen.flow().
    loaded_model.fit_generator(
        train_generator,
        steps_per_epoch=nb_train_samples // batch_size,
        epochs=epochs,
        validation_data=validation_generator,
        validation_steps=nb_validation_samples // batch_size,
        callbacks=[EarlyStopping(monitor='val_loss', patience=patience),
                   ModelCheckpoint(filepath=inception_h5_check_point, verbose=1, save_best_only=True)],
        class_weight=class_weight)

    # save weights
    loaded_model.save_weights(inception_h5_save_to)





def preprocess_input(x):
    """
    Preprocessing step for inception v3.
    """
    x /= 255.
    x -= 0.5
    x *= 2.
    return x




def get_class_weights(y, smooth_factor=0):
    """
    Returns the weights for each class based on the frequencies of the samples
    :param smooth_factor: factor that smooths extremely uneven weights
    :param y: list of true labels (the labels must be hashable)
    :return: dictionary with the weight for each class
    """
    counter = Counter(y)

    if smooth_factor > 0:
        p = max(counter.values()) * smooth_factor
        for k in counter.keys():
            counter[k] += p

    majority = max(counter.values())

    return {clss: float(majority / cnt) for clss, cnt in counter.items()}





def train_all(df_train, df_val,
              model_dir="../data/inaturalist/models/",
              train_dir="../data/inaturalist/train_images",
              val_dir="../data/inaturalist/val_images",
              verbose=1):
    """
    Train an Inception V3.
    """

    n_classes = len(set(df_train.category_id))

    ### Create model
    if verbose >= 1: print("Instantiating Inception V3...")
    base_model, model = instantiate(n_classes, inception_json=model_dir+"inceptionv3_mod.json", verbose=verbose)

    ### Train model
    if verbose >= 1: print("Fine-tuning Inception V3 first two passes...")
    finetune(base_model, model, train_dir, val_dir, batch_size=512,
             nb_train_samples=len(df_train.index), nb_validation_samples=len(df_val.index),
             patience_1=1, patience_2=2, epochs_1=15, epochs_2=30,
             inception_h5_1=model_dir+"inceptionv3_fine_tuned_1.h5",
             inception_h5_2=model_dir+"inceptionv3_fine_tuned_2.h5",
             inception_h5_check_point_1=model_dir+"inceptionv3_fine_tuned_check_point_1.h5",
             inception_h5_check_point_2=model_dir+"inceptionv3_fine_tuned_check_point_2.h5",
             layer_names_file=model_dir+"inceptionv3_mod_layer_names.txt",
             class_imbalance=True, verbose=verbose)

    if verbose >= 1: print("Fine-tuning Inception V3 third pass...")
    finetune_from_saved(model_dir+"inceptionv3_fine_tuned_check_point_2.h5",
                        model_dir+"inceptionv3_fine_tuned_3.h5",
                        model_dir+"inceptionv3_mod.json",
                        train_dir, val_dir, batch_size=512,
                        patience=5, epochs=1000,
                        nb_train_samples=len(df_train.index), nb_validation_samples=len(df_val.index),
                        inception_h5_check_point=model_dir+"inceptionv3_fine_tuned_check_point_3.h5",
                        class_imbalance=True, verbose=verbose)




#==============================================
#                   Main
#==============================================
if __name__ == '__main__':
    df_train = pd.read_csv("../data/inaturalist/train2017.csv")
    df_val = pd.read_csv("../data/inaturalist/val2017.csv")
    train_all(df_train, df_val)
